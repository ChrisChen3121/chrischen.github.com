#+TITLE: Machine Learning
#+KEYWORDS: machine learning
#+OPTIONS: H:4 toc:3 num:4 ^:nil
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Definition
  A computer program is said to learn from experience *E* with respect to some task *T*
  and some performance measure *P*, if its performance on *T*, as measured by *P*, impoves
  with experience *E* -- Tom Mitchell(1998)

* Supervised
  Teach the computer how to do something
  - "right answers" given

** Regression problem
   predict continuous valued output
    #+ATTR_HTML: align="center"
   file:../resources/ml/RegressionModel.png
*** Linear Regression
**** Hypothesis
     $$h_\theta(x)=\theta_0+\theta_1 x$$

**** Cost Function
     Called *Squared error function* or *Mean squared error*
     $$J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^m(\hat{y_i}-y_i)^2=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2$$
     - Goal: minimize $J(\theta_0, \theta_1)$
     - $\frac{1}{2}$ is a convenience for the computation of the gradient descent,
     as the derivative term of the square function will cancel out the $\frac{1}{2}$ term.

*** Gradient Descent
    To find the minimum value of the cost function.
    1. Start with some parameters $\theta_0, \theta_1, \dots$
    2. Keep changing parameters to reduce $J(\theta_0, \theta_1, \dots)$

** Classification problem
   predict discrete valued output

* Unsupervised
  Let computer learn by itself

** Clustering problem
*** Examples
    - Social network analysis
    - Market segmentation(categorize customers)
    - Astronomical data analysis


** Non-clustering problem
*** Cocktail party algorithm

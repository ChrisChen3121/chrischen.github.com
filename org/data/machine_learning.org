#+TITLE: Machine Learning
#+KEYWORDS: machine learning
#+OPTIONS: H:4 toc:3 num:4 ^:nil
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Definition
  A computer program is said to learn from experience *E* with respect to some task *T*
  and some performance measure *P*, if its performance on *T*, as measured by *P*, impoves
  with experience *E* -- Tom Mitchell(1998)

* Roadmap
   #+ATTR_HTML: align="center"
   file:../resources/ml/roadmap.png

* Supervised
  Teach the computer how to do something
  - "right answers" given

** Regression problem
   predict continuous valued output
*** Linear Regression
**** Hypothesis
     $$h_\theta(x_0, x_1, \dots)=\theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n (x_0 = 1)$$
***** vectorized
      $$h_\theta(x)=\theta^T \cdot x$$
      - $\theta$ and $x$ are $n\times 1$ matrices or $n$ vectors

**** Cost Function
     Called *Squared error function* or *Mean squared error*
     $$J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^m(\hat{y^i}-y^i)^2=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$
     - Goal: minimize $J(\theta_0, \theta_1)$
     - $\frac{1}{2}$ is a convenience for the computation of the gradient descent,
     as the derivative term of the square function will cancel out the $\frac{1}{2}$ term.
***** vectorized
      $$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2=\frac{1}{2m}\sum_{i=1}^m (\theta^T x^i-y^i)^2$$

*** Gradient Descent
    To find the minimum value of the cost function.
    1. Start with some parameters $\theta_0, \theta_1, \dots$
    2. Keep changing parameters to reduce $J(\theta_0, \theta_1, \dots)$
**** idea
     repeat until convergence
     $$\theta_i=\theta_i-\alpha\frac{d}{d\theta_i}J(\theta_0, \theta_1, \dots)$$
     - $\alpha$ too smaller: gradient descent can be slow
     - $\alpha$ too large: gradient descent can overshoot the minimum
     - Solve equation:
       $$\theta_0 = \theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^i)-y^i)$$
       $$\theta_1 = \theta_1-\alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^i)-y^i)x^i)$$
***** vectorized
      repeat until convergence
      $$\begin{align*}
      \theta_j & = \theta_j-\alpha\frac{d}{d\theta_j}J(\theta) \\
      & = \theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^i)-y^i)x_j^i
      \end{align*}$$

**** Batch Gradient Descent
     Each step of gradient descent uses all the training examples.

**** Other Method to find min value of the cost function
     Normal Equations Method
     - Gradient Descent scales better to large data sets
**** Feature Scaling
     - mean normalization
       $$\frac{x_i - \mu}{range}$$
       range could be replaced with std

**** Learning Rate

** Classification problem
   predict discrete valued output

* Unsupervised
  Let computer learn by itself

** Clustering problem
*** Examples
    - Social network analysis
    - Market segmentation(categorize customers)
    - Astronomical data analysis

** Non-clustering problem
*** Cocktail party algorithm

#+TITLE: Machine Learning
#+KEYWORDS: Machine Learning
#+OPTIONS: H:2 toc:2 num:3 ^:nil
#+OPTIONS: LaTeX:t
#+SETUPFILE: ../configOrg/level1.org
* Definition
#+BEGIN_VERSE
A computer program is said to learn from experience /E/
with respect to some task /T/ and some performance measure /P/ ,
if its performance on /T/ , as measured by P, improves with 
experience E.
#+END_VERSE
* Linear Regression
- Hypothesis
  $$h_\theta(x)=\theta^T x$$

- Cost Function
  $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$

- Goal
  $$minimize J(\theta_0, \theta_1)$$
 
** Gradient Descent
*** Outline
- Start with some $\theta$
- Keep changing $\theta$ to reduce $J(\theta)$, until we hopefully end up at minimum

*** Algorithm
#+BEGIN_VERSE
repeat until convergence {
    $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)$$ 
}
$\alpha$ is called learning rate. 
After derivative,
$$\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$
simultaneously updata $\theta_j$ for all j
#+END_VERSE

**** Simultaneous updata:
$$temp0 := \theta_0 - \alpha\frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1)$$
$$temp1 := \theta_1 - \alpha\frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1)$$
$$\theta_0 := temp0$$
$$\theta_1 := temp1$$

** Normal Equation
$$\theta = (X^TX)^{-1}X^Ty$$
*** Inverse in Octave
#+BEGIN_VERSE
pinv vs. inv
pinv is a pseudo-inverse, always make correct result even the matrix is non-invertable.
#+END_VERSE
- Why non-invertable?
  - Redundant features(linearly dependant).
  - Too many features
    #+BEGIN_VERSE
    Delete some features, or use [[regularization]]
    #+END_VERSE

*** vs. Gradient Descent
| Gradient Descent              | Normal Equation                           |
|-------------------------------+-------------------------------------------|
| Need to choose $\alpha$       | No need to choose $\alpha$                |
| Needs more iterations         | Don't need to iterate                     |
| Works well with many features | n features, $(X^TX)^{-1}$, takes $O(n^3)$ |
Number of features n > 10000, use gradient descent.


* Logistic Regression
- Hypothesis
  $$h_\theta(x)=g(\theta^T x) = \frac{1}{1+e^{-\theta^T x}}$$
  Note that, funtion g(x) is called logistic function or sigmoid function.

* Regularization
#<<regularization>>

* Octave Usage
| Function | Description     |
|----------+-----------------|
| eye()    | identity matrix |
| plot()   | generate graph  |

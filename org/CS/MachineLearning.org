#+TITLE: Machine Learning
#+KEYWORDS: Machine Learning
#+OPTIONS: H:2 toc:2 num:3 ^:nil
#+OPTIONS: LaTeX:t
#+SETUPFILE: ../configOrg/level1.org
* Definition
#+BEGIN_VERSE
A computer program is said to learn from experience /E/
with respect to some task /T/ and some performance measure /P/ ,
if its performance on /T/ , as measured by P, improves with 
experience E.
#+END_VERSE
* Linear Regression
- Hypothesis
  $$h_\theta(x)=\theta_0 + \theta_1x$$

- Parameters
  $$\theta_0, \theta_1$$

- Cost Function
  $$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
  Also called Squared error function.

- Goal
  $$minimize J(\theta_0, \theta_1)$$
 
** Gradient Descent
*** Outline
- Start with some $\theta_0, \theta_1$
- Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$
  until we hopefully end up at minimum

*** Algorithm
#+BEGIN_VERSE
repeat until convergence {
    $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)$$ 
    (for j=0 and j=1)
}
$\alpha$ is called learning rate.
#+END_VERSE

**** Simultaneous updata:
$$temp0 := \theta_0 - \alpha\frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1)$$
$$temp1 := \theta_1 - \alpha\frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1)$$
$$\theta_0 := temp0$$
$$\theta_1 := temp1$$


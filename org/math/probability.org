#+TITLE: Probability
#+KEYWORDS: math, probability
#+OPTIONS: H:3 toc:2 num:3 ^:nil
#+LaTeX: t
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Basic conceptions
** Terms
*** experiment
    Every probabilistic model involves an underlying process, called the *experiment*.

*** sample space
    The set of all possible outcomes is called the *sample space* of the experiment, denoted by $\Omega$.

*** event
    A subset of the **sample space*, that is, a collection of possible outcomes is called an *event*.

*** observation
    The outcome of each event is called *observation*.

** Set
   A *set* is a collection of objects, which are the *elements* of the set.
*** countable $S=\{x_1, x_2, \cdots\}$
*** uncountable $S=\{x|x\ satisfies\ P\}$
    ex:
    $$\{k|k/2\ is\ integer\}$$
    $$\{x|0\le x\le 1\}$$
*** complement
    The complement of a set S, with respect to the universe $\Omega$, is the set $\{x\in\Omega|x\notin S\}$
    of all elements of $\Omega$ that do not belong to S, and is denoted by $S^c$. Note that $\Omega^c = \emptyset$.
*** union
    $$S\cup T = \{x|x\in S\ or\ x\in T\}$$
    $$\bigcup_{n=1}^{\infty} = S_1\cup S_2 \cup \cdots = \{x|x\in S_n\ for\ some\ n\}$$
*** intersection
    $$S\cap T = \{x|x\in S\ and\ x\in T\}$$
    $$\bigcap_{n=1}^{\infty} = S_1\cap S_2 \cap \cdots = \{x|x\in S_n\ for\ all\ n\}$$
*** difference
    $$S\setminus T=S\cap T^c$$
    $$S\setminus T=S\setminus (S\cap T)$$
*** scalar set
    The set of scalars (real numbers) is denoted by $\Re$,
    The two-dimensional plane is denoted by $\Re_2$
*** algebra of set
**** Commutative
     $$S\cap T=T\cap S$$
     $$S\cup T=T\cup S$$

**** Associative
     $$S\cap(T\cap U)=(S\cap T)\cap U$$
     $$S\cup(T\cup U)=(S\cup T)\cup U$$

**** Distributive
     $$S\cap(T\cup U)=(S\cap T)\cup(S\cap U)$$
     $$S\cup(T\cap U)=(S\cup T)\cap(S\cup U)$$

**** de Morgan’s laws:
     $$(\bigcup_n S_n)^c=\bigcap_n S_n^c$$
     $$(\bigcap_n S_n)^c=\bigcup_n S_n^c$$

**** others
     $$(S^c)^c=S$$
     $$S\cap S^c=\emptyset$$
     $$S\cup\Omega=\Omega$$
     $$S\cap\Omega=S$$
     $$(S\setminus T)\cup T = S\cup T$$

** Axioms
*** Nonnegativity
    For every event A,
    $$P(A) \ge 0$$

*** Normalization
    The probability of the entire sample space $\omega$ is equal to 1.
    $$P(\Omega) = 1$$

*** Additivity
    If A and B are two disjoint events, then the probability of their union satisfies.
    $$P(A\cup B)=P(A)+P(B)$$
    or $A_1, A_2, \cdots$ are disjoint events,
    $$P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$$

** Consequences
*** The probability of the empty set
    $$1=P(\Omega)=P(\Omega\cup\emptyset)=P(\Omega)+P(\emptyset)=1+P(\emptyset)$$
    $$\therefore P(\emptyset)=0$$

*** Monotonicity
    If $A\subseteq B$, then $P(A)\le P(B)$

*** Addition law
    $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
**** proof
     $$P(A) = P(A\cap B) + P(A\setminus B)$$
     $$P(B) = P(B\cap A) + P(B\setminus A)$$
     $$P(A)+P(B) = 2P(A\cap B) + P(A\setminus B) + P(B\setminus A)$$
     $$P(A)+P(B) = P(A\cap B) + P((A\setminus B)\cup(B\setminus A)\cup(A\cap B))\ (axioms 3)$$
     $$P(A)+P(B) = P(A\cap B) + P(A\cup B)$$

*** others
   - $P(A\cup B)\le P(A)+P(B)$ (addition law & axioms 1)
   - $P(A\cup B\cup C) = P(A) + P(A^c\cap B) + P(A^c\cap B^c\cap C)$ (axioms 3)
   - if $P(A\cap B)$ equals to 0, then A and B are mutually exclusive

* Conditional probabilities
** Conceptions
*** $P(A|B)$
    The conditional probability of *A* given *B*, Ex:
    #+BEGIN_SRC dot :file ../resources/math/probabilityTree.png :cmdline -Kdot -Tpng
      graph probabilityTree{
        size="2,2";
        node [shape=circle fontsize=14 width=0.1 fontname="Inconsolata"];
        "begin" -- "A" [label="0.5"];
          "A" -- "B" [label="0.3"];
          "A" -- "C" [label="0.7"];
        "begin" -- "D" [label="0.5"];
          "D" -- "E" [label="1"];
      }
    #+END_SRC

    #+RESULTS:
    [[file:../resources/math/probabilityTree.png]]

    then, $P(B|A)=0.3$

*** $P(A|B) = \frac{P(A\cap B)}{P(B)}$
    - useful restatement: $P(A\cap B)=P(A|B)P(B)$
** Axioms
*** Nonnegativity
*** Normalization
    $$P(B|B)=\frac{P(B)}{P(B)}=1$$

*** Additivity
    If $A_1, A_2, \cdots$ are disjoint events,
    $$P(\bigcup_{i=1}^n A_i|B) = \sum_{i=1}^n P(A_i|B)$$

** Consequences
*** Multiplication Rule
    $$P(\cap_{i=1}^{n}A_i)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P(A_n|\cap_{i=1}^{n-1}A_i)=\prod_{i=1}^n P(A_n|\cap_{i=1}^{n-1}A_i)$$
    - proof
      $$P(\cap_{i=1}^n A_i)=P(A_1)\frac{P(A_1\cap A_2)}{P(A_1)}\cdots\frac{P(\cap_{i=1}^n A_i)}{P(\cap_{i=1}^{n-1} A_i)}$$
      $$=P(A_1)P(A_2|A_1)\cdots P(A_n|\cap_{i=1}^{n-1} A_i)$$

** Total Probability Theorem
   Let $A_1, A_2,\cdots, A_n$ be *disjoint* events that form a partition of the sample space,
   then for any event B:
   $$P(B)=P(A_1\cap B)+\cdots+P(A_n\cap B)$$
   $$=P(A_1)P(B|A_1)+\cdots+P(A_n)P(B|A_n)$$

** Bayes’ Rule
   - Useful for finding reverse conditional probabilities.
   Let $A_1, A_2,\cdots, A_n$ be *disjoint* events that form a partition of the sample space,
   then for any event B:
   $$P(A_i\cap B)=P(A_i|B)P(B)=P(A_i)P(B|A_i)$$
   $$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}$$
   - depends on total probability theorem, we have:
   $$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+\cdots+P(A_n)P(B|A_n)}$$

*** two events
    $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

** Independence
    if *A* and *B* are independent events.
    $$P(A|B)=P(A)$$
    is equivalent to
    $$P(A\cap B)=P(A)P(B)$$
    - If $A$ and $B$ are independent, so are $A$ and $B^c$
*** more events
    $$P(\bigcap_{i=1}^n A_i)=\prod_{i=1}^n P(A_i)$$
** Conditional Independence
   Two events *A* and *B* are said to be conditionally independent
   $$P(A\cap B|C)=P(A|C)P(B|C)$$
   is equivalent to(hint: *Bayes' rule*)
   $$P(A|B\cap C)=P(A|C)$$

* Counting
  - Permutations of n objects: $n!$
  - k-permutations of n objects: $\frac{n!}{(n-k)!}$
  - Combinations of k out of n objects: ${n\choose k}=\frac{n!}{k!(n-k)!}$
  - Partitions of $n$ objects into $r$ groups with /i/ th group having $n_i$ objests:

    $${n \choose n_1,n_2,\cdots,n_r} = \frac{n!}{n_1!n_2!\cdots n_r!}$$
    this is called *multinomial coefficient*

** /n/ balls into /m/ boxes
   - ball same, box same -> enum
   - ball same, box diff -> partition
     - box not null: ${n-1 \choose k-1}$
     - box nullable: ${n+k-1 \choose k-1}$

   [[https://en.wikipedia.org/wiki/Twelvefold_way][detail]]

* Probability distribution
** Conceptions
   - *random variable* is a variable that can takes on a set of values
   - *discrete* if a variable is discrete, that means it can only take exact values.
   - *expectation*
     $$E(X)=\mu=\sum_{i=1}^n x_i P(X=x_i)$$
   - *variance*
     $$Var(X)=E((X-\mu)^2)=\sum_{i=1}^n (x_i-\mu)^2 P(X=x_i)$$
   - *standard deviation*
     $$\sigma=\sqrt{Var(X)}$$
** Linear Transforms
   *Linear transforms* are when a variable X is transformed into aX + b, where a and b are constants.
   The probabilities of each Y should be the same as X
   $$E(aX+b)=aE(X)+b$$
   $$Var(aX+b)=a^2Var(X)$$

** Independent observations
   $$E(X_1+X_2+...X_n) = nE(X)$$
   $$Var(X_1+X_2+...X_n) = nVar(X)$$

** Independent Variables
   X and Y are independent random variables
   $$E(X+Y)=E(X)+E(Y)$$
   $$E(X-Y)=E(X)-E(Y)$$
   $$Var(X+Y)=Var(X)+Var(Y)$$
   $$Var(X-Y)=Var(X)-Var(Y)$$
   - linear transforms
     $$E(aX+bY)=aE(X)+bE(Y)$$
     $$E(aX-bY)=aE(X)-bE(Y)$$
     $$Var(aX+bY)=a2Var(X)+b2Var(Y)$$
     $$Var(aX-bY)=a2Var(X)-b2Var(Y)$$

** distributions
*** geometric distribution
    $$X\sim Geo(p)$$
    X follows a geometric distribution where the probability of success is p.

    1. You run a series of independent trials.
    2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
    3. The main thing you’re interested in is how many trials are needed in order to get the first successful outcome.

**** P(X)
     let X be the number of trials needed to get the first successful outcome.
     To find the probability of X taking a particular value r, using:
     $$P(X=r)=pq^{r-1}$$
     where p is the probability of success, and $q=1-p$
     $$P(X>r)=q^r$$
     $$P(X<=r)=1-q^r$$

**** E(X)
     $$E(X)=\frac{1}{p}$$

**** Var(X)
     $$Var(X)=\frac{q}{p^2}$$

**** when to use
     Use the Geometric distribution if you’re running independent trials, each one can have a success or failure, and
     you’re interested in how many trials are needed to get the first successful outcome

*** binomial distribution
    $$X\sim B(n, p)$$
    where p is the probability of a successful outcome in each trial, and n is the number of trials

    1. You’re running a series of independent trials.
    2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
    3. There are a finite number of trials.

**** P(X)
     $$P(X=r)=\dbinom{n}{r}\times p^r \times q^{n-r}$$

**** E(X)
     $$E(X)=np$$

**** Var(X)
     $$E(X)=npq$$

**** when to use
     Use the binomial distribution if you’re running a fixed number of independent trials, each one can have a success
     or failure, and you’re interested in the number of successes or failures

*** poisson distribution
    $$X\sim Po(\lambda)$$
    $\lambda$ represents the mean number of occurrences.

    1. Individual events occur at random and independently in a given interval.
    2. You know the mean number of occurrences in the interval or the rate of occurrences, and it’s finite.

**** P(X)
     $$P(X)=\frac{e^{-\lambda}\lambda^{r}}{r!}$$

**** E(X)
     $$E(X)=\lambda$$

**** Var(X)
     $$Var(X)=\lambda$$

**** linear transforms
     If $X\sim Po(\lambda_x)$ and $Y\sim Po(\lambda_y)$,
     $$X+Y\sim Po(\lambda_x + \lambda_y)$$

**** when to use
     Use the Poisson distribution if you have independent events such as malfunctions occurring in a given interval,
     and you know λ, the mean number of occurrences in a given interval. You’re interested in the number of
     occurrences in one particular interval.

**** other use
     if *np* is like *npq*, *q* should be close to 1, thus

     $X\sim B(n, p)$ can be approximated by $X\sim Po(np)$ if n is large and p is small.

*** normal distribution
    $$X\sim N(\mu, \sigma^2)$$
**** probability density function
     $$f(X)=\frac{1}{\sigma\sqrt{2\pi}^e}^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

**** standard score
     $$z=\frac{x-\mu}{\sigma}$$

**** probability $P(X\le x)$
     $$P(X\le c) = \int_{-\inf}^{c}f(x)dx=\frac{1}{\sigma\sqrt{2\pi}}\int_{-\inf}^{c}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}dx$$

**** linear transforms
     - $X\pm Y\sim N(\mu_x\pm \mu_y, \sigma_x^2+\sigma_y^2)$ if $X\sim N(\mu_x, \sigma_x^2)$ and $Y\sim N(\mu_y, \sigma_y^2)$ are independent
     - $aX+b\sim N(a\mu+b, a^2\sigma^2)$
     - if $X_1,X_2,...,X_n$ are independent observations of X, then $X_1+X_2+...+X_n\sim N(n\mu, n\sigma^2)$

**** approximate the Binomial
     - if $X\sim B(n, p)$ and $np>5$ and $nq>5$, use $X\sim N(np, npq)$ to approximate it
     - also need a *continuity correction* adjustment. (round to discrete value)

**** approximate the Poisson
     - if $X\sim Po(\lambda)$ and $\lambda>15$, use $X\sim N(\lambda, \lambda)$ to approximate it

* Binomial Theorem
   $$(x+y)^n = \sum_{k=0}^{n} \dbinom{n}{k}x^{n-k}y^{k}$$

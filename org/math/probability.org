#+TITLE: Probability
#+KEYWORDS: math, probability
#+OPTIONS: H:3 toc:2 num:3 ^:nil
#+LaTeX: t
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Basic conceptions
** Terms
*** experiment
    Every probabilistic model involves an underlying process, called the *experiment*.

*** sample space
    The set of all possible outcomes is called the *sample space* of the experiment, denoted by $\Omega$.

*** event
    A subset of the **sample space*, that is, a collection of possible outcomes is called an *event*.

*** observation
    The outcome of each event is called *observation*.

** Set
   A *set* is a collection of objects, which are the *elements* of the set.
*** countable $S=\{x_1, x_2, \cdots\}$
*** uncountable $S=\{x|x\ satisfies\ P\}$
    ex:
    $$\{k|k/2\ is\ integer\}$$
    $$\{x|0\le x\le 1\}$$
*** complement
    The complement of a set S, with respect to the universe $\Omega$, is the set $\{x\in\Omega|x\notin S\}$
    of all elements of $\Omega$ that do not belong to S, and is denoted by $S^c$. Note that $\Omega^c = \emptyset$.
*** union
    $$S\cup T = \{x|x\in S\ or\ x\in T\}$$
    $$\bigcup_{n=1}^{\infty} = S_1\cup S_2 \cup \cdots = \{x|x\in S_n\ for\ some\ n\}$$
*** intersection
    $$S\cap T = \{x|x\in S\ and\ x\in T\}$$
    $$\bigcap_{n=1}^{\infty} = S_1\cap S_2 \cap \cdots = \{x|x\in S_n\ for\ all\ n\}$$
*** difference
    $$S\setminus T=S\cap T^c$$
    $$S\setminus T=S\setminus (S\cap T)$$
*** scalar set
    The set of scalars (real numbers) is denoted by $\Re$,
    The two-dimensional plane is denoted by $\Re_2$
*** algebra of set
**** Commutative
     $$S\cap T=T\cap S$$
     $$S\cup T=T\cup S$$

**** Associative
     $$S\cap(T\cap U)=(S\cap T)\cap U$$
     $$S\cup(T\cup U)=(S\cup T)\cup U$$

**** Distributive
     $$S\cap(T\cup U)=(S\cap T)\cup(S\cap U)$$
     $$S\cup(T\cap U)=(S\cup T)\cap(S\cup U)$$

**** de Morgan’s laws:
     $$(\bigcup_n S_n)^c=\bigcap_n S_n^c$$
     $$(\bigcap_n S_n)^c=\bigcup_n S_n^c$$

**** others
     $$(S^c)^c=S$$
     $$S\cap S^c=\emptyset$$
     $$S\cup\Omega=\Omega$$
     $$S\cap\Omega=S$$
     $$(S\setminus T)\cup T = S\cup T$$

** Axioms
*** Nonnegativity
    For every event A,
    $$P(A) \ge 0$$

*** Normalization
    The probability of the entire sample space $\omega$ is equal to 1.
    $$P(\Omega) = 1$$

*** Additivity
    If A and B are two disjoint events, then the probability of their union satisfies.
    $$P(A\cup B)=P(A)+P(B)$$
    or $A_1, A_2, \cdots$ are disjoint events,
    $$P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$$

** Consequences
*** The probability of the empty set
    $$1=P(\Omega)=P(\Omega\cup\emptyset)=P(\Omega)+P(\emptyset)=1+P(\emptyset)$$
    $$\therefore P(\emptyset)=0$$

*** Monotonicity
    If $A\subseteq B$, then $P(A)\le P(B)$

*** Addition law
    $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
**** proof
     $$P(A) = P(A\cap B) + P(A\setminus B)$$
     $$P(B) = P(B\cap A) + P(B\setminus A)$$
     $$P(A)+P(B) = 2P(A\cap B) + P(A\setminus B) + P(B\setminus A)$$
     $$P(A)+P(B) = P(A\cap B) + P((A\setminus B)\cup(B\setminus A)\cup(A\cap B))\ (axioms 3)$$
     $$P(A)+P(B) = P(A\cap B) + P(A\cup B)$$

*** others
   - $P(A\cup B)\le P(A)+P(B)$ (addition law & axioms 1)
   - $P(A\cup B\cup C) = P(A) + P(A^c\cap B) + P(A^c\cap B^c\cap C)$ (axioms 3)
   - if $P(A\cap B)$ equals to 0, then A and B are mutually exclusive

** Random Variable
   *random variable* is a variable that can takes on a set of values
** Discrete
   *discrete* if a variable is discrete, that means it can only take exact values.
** Expectation
   $$E(X)=\mu=\sum_{i=1}^n x_i P(X=x_i)$$
** Variance
   $$Var(X)=E((X-\mu)^2)=\sum_{i=1}^n (x_i-\mu)^2 P(X=x_i)$$
** Standard Deviation
   $$\sigma=\sqrt{Var(X)}$$

* Conditional probabilities
** Conceptions
*** $P(A|B)$
    The conditional probability of *A* given *B*, Ex:
    #+BEGIN_SRC dot :file ../resources/math/probabilityTree.png :cmdline -Kdot -Tpng
      graph probabilityTree{
        size="2,2";
        node [shape=circle fontsize=14 width=0.1 fontname="Inconsolata"];
        "begin" -- "A" [label="0.5"];
          "A" -- "B" [label="0.3"];
          "A" -- "C" [label="0.7"];
        "begin" -- "D" [label="0.5"];
          "D" -- "E" [label="1"];
      }
    #+END_SRC

    #+RESULTS:
    [[file:../resources/math/probabilityTree.png]]

    then, $P(B|A)=0.3$

*** $P(A|B) = \frac{P(A\cap B)}{P(B)}$
    - useful restatement: $P(A\cap B)=P(A|B)P(B)$
** Axioms
*** Nonnegativity
*** Normalization
    $$P(B|B)=\frac{P(B)}{P(B)}=1$$

*** Additivity
    If $A_1, A_2, \cdots$ are disjoint events,
    $$P(\bigcup_{i=1}^n A_i|B) = \sum_{i=1}^n P(A_i|B)$$

** Consequences
*** Multiplication Rule
    $$P(\cap_{i=1}^{n}A_i)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P(A_n|\cap_{i=1}^{n-1}A_i)=\prod_{i=1}^n P(A_n|\cap_{i=1}^{n-1}A_i)$$
    - proof
      $$P(\cap_{i=1}^n A_i)=P(A_1)\frac{P(A_1\cap A_2)}{P(A_1)}\cdots\frac{P(\cap_{i=1}^n A_i)}{P(\cap_{i=1}^{n-1} A_i)}$$
      $$=P(A_1)P(A_2|A_1)\cdots P(A_n|\cap_{i=1}^{n-1} A_i)$$

** Total Probability Theorem
   Let $A_1, A_2,\cdots, A_n$ be *disjoint* events that form a partition of the sample space,
   then for any event B:
   $$P(B)=P(A_1\cap B)+\cdots+P(A_n\cap B)$$
   $$=P(A_1)P(B|A_1)+\cdots+P(A_n)P(B|A_n)$$

** Bayes’ Rule
   - Useful for finding reverse conditional probabilities.
   Let $A_1, A_2,\cdots, A_n$ be *disjoint* events that form a partition of the sample space,
   then for any event B:
   $$P(A_i\cap B)=P(A_i|B)P(B)=P(A_i)P(B|A_i)$$
   $$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}$$
   - depends on total probability theorem, we have:
   $$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+\cdots+P(A_n)P(B|A_n)}$$

*** two events
    $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

** Independence
    if *A* and *B* are independent events.
    $$P(A|B)=P(A)$$
    is equivalent to
    $$P(A\cap B)=P(A)P(B)$$
    - If $A$ and $B$ are independent, so are $A$ and $B^c$
*** more events
    $$P(\bigcap_{i=1}^n A_i)=\prod_{i=1}^n P(A_i)$$
** Conditional Independence
   Two events *A* and *B* are said to be conditionally independent
   $$P(A\cap B|C)=P(A|C)P(B|C)$$
   is equivalent to(hint: *Bayes' rule*)
   $$P(A|B\cap C)=P(A|C)$$

* Random Variables
  - difinition: A *random variable* is a real-valued function of the outcome of the experiment
  - A *function of a random variable* defines another random variable
** Discrete RV
  - A (discrete) random variable has an associated probability mass function(PMF)
*** PMF
    Probability mass function,
    $$p_X(x)=P(X=x)$$
    Note that:
    $$\sum_x p_X(x)=1$$
*** CDF
    $$F_X(x)=P(X\le x)=\sum_{k\le x}p_X(k)$$
** Continuous RV
*** CDF
    $$F_X(x)=P(X\le x)=\int_{-\infty}^x f_X(t)dt$$

* Counting
  - Permutations of n objects: $n!$
  - k-permutations of n objects: $\frac{n!}{(n-k)!}$
  - Combinations of k out of n objects: ${n\choose k}=\frac{n!}{k!(n-k)!}$
  - Partitions of $n$ objects into $r$ groups with /i/ th group having $n_i$ objests:

    $${n \choose n_1,n_2,\cdots,n_r} = \frac{n!}{n_1!n_2!\cdots n_r!}$$
    this is called *multinomial coefficient*

** /n/ balls into /m/ boxes
   - ball same, box same -> enum
   - ball same, box diff -> partition
     - box not null: ${n-1 \choose k-1}$
     - box nullable: ${n+k-1 \choose k-1}$

   [[https://en.wikipedia.org/wiki/Twelvefold_way][detail]]

* Linear Transforms
  *Linear transforms* are when a variable X is transformed into aX + b, where a and b are constants.
  The probabilities of each Y should be the same as X
  $$E(aX+b)=aE(X)+b$$
  $$Var(aX+b)=a^2Var(X)$$

** Independent observations
   $$E(X_1+X_2+...X_n) = nE(X)$$
   $$Var(X_1+X_2+...X_n) = nVar(X)$$

** Independent Variables
   X and Y are independent random variables
   $$E(X+Y)=E(X)+E(Y)$$
   $$E(X-Y)=E(X)-E(Y)$$
   $$Var(X+Y)=Var(X)+Var(Y)$$
   $$Var(X-Y)=Var(X)-Var(Y)$$
   - linear transforms
     $$E(aX+bY)=aE(X)+bE(Y)$$
     $$E(aX-bY)=aE(X)-bE(Y)$$
     $$Var(aX+bY)=a2Var(X)+b2Var(Y)$$
     $$Var(aX-bY)=a2Var(X)-b2Var(Y)$$

* Discrete distributions
** Bernoulli distribution
   Let $X$ be a discrete random variable. Let its support be $R_X = \{0, 1\}$, the probability of $X=1$ is $p$

   $$X\sim Bernoulli(p)$$
*** PMF
$$P_X(x)= \begin{cases}
p,  & \mbox{if }x = 1 \\
1-p, & \mbox{if }x = 0 \\
0, & \mbox{if }x\notin R_X \\
\end{cases}$$

*** E(X)
    $$E(X)=p$$

*** Var(X)
    $$Var(X)=p(1-p)$$

*** CDF
$$F_X(x)= \begin{cases}
0,  & \mbox{if }x \le 0\\
1-p, & \mbox{if }0 \le x < 1 \\
1, & \mbox{if }x\ge 1 \\
\end{cases}$$

** Binomial distribution
   1. You’re running a series of *independent* trials.
   2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
   3. There are a *finite* number of trials.
   4. The main thing you’re interested in is *the number of successes* in the $n$ independent trials.

   Let:
   - $X$ be the number of successful outcomes out of $n$ trials
   - $p$ be the probability of success in a trial

   $$X\sim B(n, p)$$

*** PMF
    $$P_X(x)=\dbinom{n}{x} p^x (1-p)^{n-x}$$

*** E(X)
    $$E(X)=np$$

*** Var(X)
    $$E(X)=np(1-p)$$

*** CDF
    $$F_X(x)=\sum_{m=-\infty}^{\lfloor x \rfloor}{n \choose m}p^m(1-p)^{n-m}$$

** Geometric distribution
   1. You run a series of *independent* trials.
   2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
   3. The main thing you’re interested in is *how many* trials are needed in order to get the *first* successful outcome.

   Let:
   - $X$ be the number of trials needed to get the first successful outcome
   - $p$ be the probability of success in a trial

   $$X\sim Geo(p)$$
*** PMF
    let X be the number of trials needed to get the first successful outcome.
    To find the probability of $X$ taking a particular value $x$, using:
    $$P_X(x)=p(1-p)^{x-1}$$

*** E(X)
    $$E(X)=\frac{1}{p}$$

*** Var(X)
    $$Var(X)=\frac{1-p}{p^2}$$

*** CDF
    $$F_X(x)=1-(1-p)^{\lfloor x \rfloor}$$

** Uniform distribution
   $$X\sim Uniform(a, b)$$
   A discrete random variable has a uniform distribution if all the values belonging to its support have the same probability density.
   Let $X$ be a discrete random variable. Let its support be $R_X=\{a, a+k, a+2k \dots b\}$. Let the number of $R_X$ be $N$, then $N=\frac{b-a}{k}+1=\frac{b-a+k}{k}$
*** PMF
    $$P_X(x)=\frac{1}{N}=\frac{k}{b-a+k}$$

*** E(X)
    $$E(X)=\frac{a+b}{2}$$

*** Var(X)
    $$Var(X)=k^2(\frac{N^2-1}{12})$$

*** CDF
$$F_X(x)= \begin{cases}
0,  & \mbox{if }x < a\\
\frac{\lfloor x\rfloor -a+k}{b-a+k}, & \mbox{if }a \le x \le b \\
1, & \mbox{if }x > b \\
\end{cases}$$

** Pascal distribution
   aslo known as *Negative binomial distribution*

   1. You run a series of *independent* trials.
   2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
   3. The main thing you’re interested in is *how many* trials are needed in order to get the $n$ successful outcomes.

   Let:
   - $X$ be the number of trials needed to get the $n$ successful outcomes
   - $p$ be the probability of success in a trial

   $$X\sim Pascal(n, p)$$

*** PMF
$$P_X(x)=\begin{cases}
{x-1 \choose n-1}(1-p)^{x-n}p^n,  & x\ge n\\
0, & otherwise\\
\end{cases}$$

*** E(X)
    $$E(X)=\frac{n}{p}$$

*** Var(X)
    $$Var(X)=\frac{n(1-p)}{p^2}$$

*** CDF
$$F_X(x)=\\
\begin{cases}
I_p(n, \lfloor x \rfloor - n + 1),  & x\ge n\\
0, & otherwise\\
\end{cases}$$
$I_z(a,b)$ is the regularized incomplete beta function
**** more
$$\begin{align*}
F_X(x) & =P(X\le x)\\
& =P(\mbox{the n-th success occurs before the x-th trial})\\
& =P(\mbox{n success in x trials})\\
& =P(Y\ge n), Y\sim B(x, p)
\end{align*}$$
This is why *pascal* is aslo called *negative binomial*.

** Poisson distribution
   1. Individual events occur at *random* and *independently* in a given interval.
   2. You know the mean number of occurrences in the interval or the rate of occurrences, and it’s finite.
   3. The purpose is to know the number of occurrences in another particular interval.

   Let:
   - $X$ be the number of occurrences in a particular interval
   - $\lambda$ be the rate of occurrences, should be $\mu T$

   $$X\sim Po(\lambda)$$

*** PMF
    $$P_X(x)=\frac{e^{-\lambda}\lambda^{x}}{x!}$$

*** E(X)
    $$E(X)=\lambda$$

*** Var(X)
    $$Var(X)=\lambda$$

*** CDF
$$F_X(x)= \begin{cases}
-\lambda^e\sum_{n=-\infty}^{\lfloor x \rfloor}e^{-\lambda}\cdot \frac{\lambda^n}{n!}, & \mbox{if }x\ge 1 \\
0, & \mbox{otherwise} \\
\end{cases}$$

*** linear transforms
    If $X\sim Po(\lambda_x)$ and $Y\sim Po(\lambda_y)$, and $X$ and $Y$ are independent,
    $$X+Y\sim Po(\lambda_x + \lambda_y)$$

*** simplify the special Binomial distribution case
    If $X\sim B(n, p)$ where $n$ is large and $p$ is small, you can approximate it with $X \sim Po(np)$.


# ** normal distribution
#    $$X\sim N(\mu, \sigma^2)$$
# *** probability density function
#     $$f(X)=\frac{1}{\sigma\sqrt{2\pi}^e}^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

# *** standard score
#     $$z=\frac{x-\mu}{\sigma}$$

# *** probability $P(X\le x)$
#     $$P(X\le c) = \int_{-\inf}^{c}f(x)dx=\frac{1}{\sigma\sqrt{2\pi}}\int_{-\inf}^{c}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}dx$$

# *** linear transforms
#     - $X\pm Y\sim N(\mu_x\pm \mu_y, \sigma_x^2+\sigma_y^2)$ if $X\sim N(\mu_x, \sigma_x^2)$ and $Y\sim N(\mu_y, \sigma_y^2)$ are independent
#     - $aX+b\sim N(a\mu+b, a^2\sigma^2)$
#     - if $X_1,X_2,...,X_n$ are independent observations of X, then $X_1+X_2+...+X_n\sim N(n\mu, n\sigma^2)$

# *** approximate the Binomial
#     - if $X\sim B(n, p)$ and $np>5$ and $nq>5$, use $X\sim N(np, npq)$ to approximate it
#     - also need a *continuity correction* adjustment. (round to discrete value)

# *** approximate the Poisson
#     - if $X\sim Po(\lambda)$ and $\lambda>15$, use $X\sim N(\lambda, \lambda)$ to approximate it

* Binomial Theorem
   $$(x+y)^n = \sum_{k=0}^{n} \dbinom{n}{k}x^{n-k}y^{k}$$

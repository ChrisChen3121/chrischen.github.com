#+TITLE: Probability
#+KEYWORDS: math, probability
#+OPTIONS: H:3 toc:2 num:3 ^:nil
#+LaTeX: t
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Probability
** Conceptions
   - *event*

   An outcome or occurrence that has a probability assigned to it.

   - *observation*

   The outcome of each event is called observation.

** Axioms
*** Nonnegativity
    For every event A, $P(A) \ge 0$.

*** Complementary events
    $$P(A)+P(A^{'})=1$$

** Laws
   - If $A\subset B$, then $P(A)\le P(B)$
   - $P(A\cup B) = P(A) + P(B) - P(A \cap B)$
   - $P(A\cup B) \le P(A)+P(B)$
   - $P(A\cup B\cup C) = P(A) + P(A^c\cap B) + P(A^c\cap B^c\cap C)$ ??
   - if $P(A\cap B)$ equals to 0, this stuation is called mutually exclusive

* Conditional probabilities
** Conceptions
*** P(A|B)
    The probability of *A* given that we know *B* has happened.
    #+BEGIN_SRC dot :file ../resources/math/probabilityTree.png :cmdline -Kdot -Tpng
      graph probabilityTree{
        size="2,2";
        node [shape=circle fontsize=14 width=0.1 fontname="Inconsolata"];
        "begin" -- "A" [label="50%"];
          "A" -- "B" [label="30%"];
          "A" -- "C" [label="70%"];
        "begin" -- "D" [label="50%"];
          "D" -- "E" [label="100%"];
      }
    #+END_SRC

    #+RESULTS:
    [[file:../resources/math/probabilityTree.png]]

    then, P(B|A)=30%

** Laws
   $$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
   $$P(A\cap B) = P(A|B)\times P(B)$$
   - law of total probability
   $$P(B)=P(B\cap A) + P(B\cap A^{'})=P(A)\times P(B|A)+P(A^{'})\times P(B|A^{'})$$
*** independent
    if *A* and *B* are independent events.
    $$P(A|B)=P(A)$$
    $$P(A\cap B) = P(A)\times P(B)$$

** Bayes' Theorem
   $$P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(A)\times P(B|A)}{P(A)\times P(B|A) + P(A^{'})\times P(B|A^{'})}$$
   use to find reverse conditional probabilities

* Probability distribution
** Conceptions
   - *random variable* is a variable that can takes on a set of values
   - *discrete* if a variable is discrete, that means it can only take exact values.
   - *expectation*
     $$E(X)=\mu=\sum_{i=1}^n x_i P(X=x_i)$$
   - *variance*
     $$Var(X)=E((X-\mu)^2)=\sum_{i=1}^n (x_i-\mu)^2 P(X=x_i)$$
   - *standard deviation*
     $$\sigma=\sqrt{Var(X)}$$
** Linear Transforms
   *Linear transforms* are when a variable X is transformed into aX + b, where a and b are constants.
   The probabilities of each Y should be the same as X
   $$E(aX+b)=aE(X)+b$$
   $$Var(aX+b)=a^2Var(X)$$

** Independent observations
   $$E(X_1+X_2+...X_n) = nE(X)$$
   $$Var(X_1+X_2+...X_n) = nVar(X)$$

** Independent Variables
   X and Y are independent random variables
   $$E(X+Y)=E(X)+E(Y)$$
   $$E(X-Y)=E(X)-E(Y)$$
   $$Var(X+Y)=Var(X)+Var(Y)$$
   $$Var(X-Y)=Var(X)-Var(Y)$$
   - linear transforms
     $$E(aX+bY)=aE(X)+bE(Y)$$
     $$E(aX-bY)=aE(X)-bE(Y)$$
     $$Var(aX+bY)=a2Var(X)+b2Var(Y)$$
     $$Var(aX-bY)=a2Var(X)-b2Var(Y)$$

** distributions
*** geometric distribution
    $$X\sim Geo(p)$$
    X follows a geometric distribution where the probability of success is p.

    1. You run a series of independent trials.
    2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
    3. The main thing you’re interested in is how many trials are needed in order to get the first successful outcome.

**** P(X)
     let X be the number of trials needed to get the first successful outcome.
     To find the probability of X taking a particular value r, using:
     $$P(X=r)=pq^{r-1}$$
     where p is the probability of success, and $q=1-p$
     $$P(X>r)=q^r$$
     $$P(X<=r)=1-q^r$$

**** E(X)
     $$E(X)=\frac{1}{p}$$

**** Var(X)
     $$Var(X)=\frac{q}{p^2}$$

**** when to use
     Use the Geometric distribution if you’re running independent trials, each one can have a success or failure, and
     you’re interested in how many trials are needed to get the first successful outcome

*** binomial distribution
    $$X\sim B(n, p)$$
    where p is the probability of a successful outcome in each trial, and n is the number of trials

    1. You’re running a series of independent trials.
    2. There can be either a success or failure for each trial, and the probability of success is the same for each trial.
    3. There are a finite number of trials.

**** P(X)
     $$P(X=r)=\dbinom{n}{r}\times p^r \times q^{n-r}$$

**** E(X)
     $$E(X)=np$$

**** Var(X)
     $$E(X)=npq$$

**** when to use
     Use the binomial distribution if you’re running a fixed number of independent trials, each one can have a success
     or failure, and you’re interested in the number of successes or failures


*** poisson distribution
    $$X\sim Po(\lambda)$$
    $\lambda$ represents the mean number of occurrences.

    1. Individual events occur at random and independently in a given interval.
    2. You know the mean number of occurrences in the interval or the rate of occurrences, and it’s finite.

**** P(X)
     $$P(X)=\frac{e^{-\lambda}\lambda^{r}}{r!}$$

**** E(X)
     $$E(X)=\lambda$$

**** Var(X)
     $$Var(X)=\lambda$$

**** linear transforms
     If $X\sim Po(\lambda_x)$ and $Y\sim Po(\lambda_y)$,
     $$X+Y\sim Po(\lambda_x + \lambda_y)$$

**** when to use
     Use the Poisson distribution if you have independent events such as malfunctions occurring in a given interval,
     and you know λ, the mean number of occurrences in a given interval. You’re interested in the number of
     occurrences in one particular interval.

**** other use
     if *np* is like *npq*, *q* should be close to 1, thus

     $X\sim B(n, p)$ can be approximated by $X\sim Po(np)$ if n is large and p is small.

*** normal distribution

* Binomial Theorem
   $$(x+y)^n = \sum_{k=0}^{n} \dbinom{n}{k}x^{n-k}y^{k}$$

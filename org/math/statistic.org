#+TITLE: Statistic
#+KEYWORDS: math, statistic
#+OPTIONS: H:3 toc:2 num:3 ^:nil
#+LaTeX: t
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Type of research
- Observational Study
- Controlled Experiment
- Survey
* Central tendency
- mean
  $$\mu = \frac{\sum_{i=1}^{n}x_i}{n}$$
  $$\mu = \frac{\sum_{i=1}^{n}freq_i \cdot x_i}{\sum_{i=1}^{n}freq_i}$$
  $$\mu = \sum_{i=1}^{n}p_i\cdot x_i$$
- mode
- median
  - even
    $$\frac{x_\frac{n}{2} + x_{\frac{n}{2}+1}}{2}$$

  - odd
    $$x_\frac{n+1}{2}$$

* Variability
** range
   highest value - lowest value

** quartile
   | lower bound(lowest) | lower quartile(Q1) | median(Q2) | upper quartile(Q3) | upper bound(highest) |

*** mini range
    may cut off the upper 25% and lower 25% tails of the distribution

*** IQR(Interquartile Range)
    $$Q_3 - Q_1$$
    $Q_1$ is the midean of the first half

    file:../resources/math/IQR.png

*** Outlier
    whiskers = 1.5
    $$< Q_1 - whiskers\cdot(IQR)$$
    $$> Q_3 + whiskers\cdot(IQR)$$

** deviation
*** MAD
    mean absolute deviation, average absolute distances to the mean

*** Variance
    $$\sigma^2 = \frac{\sum_{i=1}^n(x_i-\mu)^2}{n}$$
    $$\sigma^2 = \frac{\sum_{i=1}^n x_i^2}{n} - \mu^2$$

*** Standard deviation
    - standard deviation $\sigma$
    - approximately 68% lie within 1SD of the mean
    - approximately 95% lie within 2SD of the mean

*** Standandize(z-score)
    $$z=\frac{x-\mu}{\sigma}$$
    Standard scores work by transforming sets of data into a new, theoretical distribution
    with a mean of 0 and a standard deviation of 1.

* Sampling Distribution
** Central limit theorem
1. The distribution of sample means is approximately normal distribution
2. The standard deviation of the sample(standard error) means $\approx\frac{\sigma}{\sqrt{n}}$
3. The mean of the sample means $\approx\mu$
** Confidence Interval
- z score:

  $z=\frac{x-\mu}{\sigma}$

- Y% confidence interval:
  $$(\bar{x}-z\frac{\sigma}{\sqrt{N}}, \bar{x}+z\frac{\sigma}{\sqrt{n}})$$
  $\pm{z}$ are the critical values of Y% confidence interval

- Margin of error $z\frac{\sigma}{\sqrt{n}}$
** Terms
  - Sample Variance

    Bessel's Correction a better estimate of population
    $$\sigma^2 = \frac{\sum_{i=1}^N(x_i-\mu)^2}{N-1}$$

| population mean       | $\mu$                                         |
| population sd         | $\sigma$                                      |
| sample mean           | $\bar{x}$                                     |
| sample sd             | $s$                                           |
| z-score               | number of standard deviations away from $\mu$ |
| sampling distribution | distribution of sample means                  |
| standard error        | the SD of the sampling distribution           |

* Hypothesis testing
- $\alpha$ levels

  Levels of likelihood: 5%, 1%, 0.1%. If the probability of getting a particular
  sample mean is less than 5%, it is unlikely to occur.

- critical region: the aera with the probability is less than particular $\alpha$ level
- critical value: the z-score of the $\alpha$ level

** null hypothesis $H_0$
- type 1 error

  Reject $H_0$, but in the real world $H_0$ is true.
  We think our hypothesis is correct, but it is wrong.

- type 2 error

  Retain $H_0$, but in the real world $H_0$ is false.
  We think our hypothesis is wrong, but it is true.

** alternative hypothesis $H_A$
- assume $H_0$ is
$$\mu\approx\mu_I$$

- then $H_A$ is
$$\mu\neq\mu_I, \mu<\mu_I, \mu>\mu_I$$
* T-Tests
  Compare the difference between two means
** SEM(standard error of mean)
   uses sample sd
   $$SEM=\frac{s}{\sqrt{n}}$$
** t-score(one sample)
   $$t=\frac{\bar{x}-\mu_0}{SEM}=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}$$
** Cohen's d
   standardized mean difference that measures the distance between means in standardized units.
   $$Cohen's\ d = \frac{\bar{x}-\mu_0}{s}$$
** $r_2$
   #+BEGIN_VERSE
   $r_2$: coefficient of determination
   $r^2$ % of variation in one variable that is related to
   ('explained by') another variable.
   #+END_VERSE
   $$r^2 = \frac{t^2}{t^2+DF}$$
** Formula Wrapup
$$DF=n-1$$
$$SEM=\frac{s}{\sqrt{n}}$$
$$t=\frac{\bar{x}-\mu}{SEM}$$
$$d=\frac{\bar{x}-\mu}{s}$$

$$margin\ of\ error = t_{criticl}\cdot{SEM}$$
$$CI=\bar{x}\pm{margin\ of\ error}$$
$$r^2=\frac{t^2}{t^2+df}$$

** Terms
| DF  | degree of freedom      |
| SEM | standard error of mean |
* Dependent 2 sample t-test
$$t=\frac{\bar{x}_D-\mu_D}{S_D/\sqrt{n}}$$
$$CI=\bar{x}_D\pm t_{critical}\cdot\frac{S_D}{\sqrt{n}}$$
$$cohen's\ d=\frac{\bar{x}_D}{S_D}$$
** Within-Subject designs
- Two conditions
- Pre-test, post-test
- Growth over time(longitudinal study)
** Effect size
- difference measures: mean, standardized
  #+BEGIN_VERSE
  cohen's d == standardized mean difference
  #+END_VERSE

** Statistical significance

   Statistical significance means:
   - rejected the null
   - results are not likely due to chance(sampling error)

** Advantages
- Controls for individual differences
- Use fewer subject
- Cost-effective
- Less time-consuming
- Less expensive

** Disadvantages
- Carry-over effects

  Second measurement can be affected by first treatment

- Order may influence results

* Independent 2 sample t-test
** Between-Subject designs
- Experimental
- Observational

** DF
   $$DF = n_1+n_2-2$$
** SE
   $$s=\sqrt{s_{1}^2+s_{2}^2}$$

   Assumes samples are approximately the same size, then
   $$SE=\sqrt{\frac{s_{1}^2}{n_1} + \frac{s_{2}^2}{n_2}}$$

*** Corrected Standard Error
    $$SS=\sum_{i=1}^{n}(x_i-\bar{x})^2$$
    $$S_{p}^2=\frac{SS_1 + SS_2}{df_1 + df_2}$$
    $$SE=\sqrt{\frac{S_{p}^2}{n_1} + \frac{S_{p}^2}{n_2}}$$

    cohen's d also uses $S_p$

    $$d=\frac{\bar{x}-\mu}{S_p}$$

** t statistic
   $$t=\frac{\bar{x}_D-\mu_D}{SE}$$

* ANOVA
  Analysis of variance
** Grand mean $\bar{x}_G$
   mean of all values

** F-Ratio
- Between-group variability

  The greater the distance between sample means, the more
  likely population means will differ significantly.

- Within-group variability

  The greater the variability of each individual sample,
  the less likely population means will differ significantly.

$$F=\frac{MS_{between}}{MS_{within}}=\frac{SS_{between}/df_{between}}{SS_{within}/df_{within}}
=\frac{\sum_{i}n_i(\bar{x}_i-\bar{x}_G)^2/(k-1)}{\sum_i\sum_j(x_{ij}-\bar{x}_i)^2/(N-k)}$$
$$SS_{total}=SS_{between}+SS_{within}=\sum_i\sum_j(x_{ij}-\bar{x}_G)$$
$$df_{total}=df_{between}+df_{within}=N-1$$

** Multiple Comparison Tests

   We use *multiple comparison tests* if we want to know which two samples
   are differ after we've done ANOVA.
*** Tukey's Honestly Significant Difference(HSD)
    $$Tukey's HSD = q^*\sqrt{\frac{MS_{within}}{n}} = q^*\frac{S_p}{n}$$
    /q/ is the *Studentized Range Statistic*

** Cohen's d
   $$Cohen's\ d = \frac{\bar{x}_a-\bar{x}_b}{MS_{within}}$$

** Explained Variation $\eta^2$
   Proportion of total variation that is due to between-group differences.
   $$\eta^2=\frac{SS_{between}}{SS_{total}}$$

** ANOVA assumptions
- Normality
- Homogeneity of variance
- Independence of observations

* Report
** Meaningfulness of Results
1. What was measured?
2. Effect size
3. Can we rule out random chance?
4. Can we rule out alternative explanations?(lurking variables)

** descriptive statistics(Mean,SD,...)

   report styles: text, graphs, tables

** inferential statistics($\alpha$)
*** factors
- kind of test
- test statistic
- DF
- p-value
- direction of test(one/two tails)

*** inferential statistics
- confidence intervals
  - confidence level e.g. 95%
  - lower limit
  - upper limit
  - CI on what?

*** effect size measures

    d, $r^2$

*** APA style

    APA style is a whole guide on writing researh papers.
    $$t(df)=xxx, p=xx, direction$$
    example:
    $$t(24)=-2.5, p=0.01, one-tailed$$

    - CI

    example: Confidence interval on the mean difference;95%CI=(4,6)
** visualization
*** Pie charts
    Pie charts work by splitting your data into distinct groups or categories.
    Pie charts are less useful if all the slices have similar sizes, use bar charts for this case.
*** Bar charts
    Bar charts allow you to compare relative sizes, but the advantage of using a bar chart is that they
    allow for a greater degree of precision.
    - vertical or horizontal

    Vertical bar charts tend to be more common, but horizontal bar charts
    are useful if the names of your categories are long.

**** extensions
     - The split-category bar chart
     - The segmented bar chart

*** Histogram
    Histograms are like bar charts but with two key differences.
    - The area of each bar is proportional to the frequency
    - There are no gaps between the bars on the chart

*** Line charts
    Line charts are better at showing a trend.

#+TITLE: Statistic
#+KEYWORDS: math, statistic
#+OPTIONS: H:3 toc:2 num:3 ^:nil
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+SETUPFILE: ../../org-templates/level-1.org
* Type of research
- Observational Study
- Controlled Experiment
- Survey

* Central tendency
** mean
   $$\mu = \frac{\sum_{i=1}^{n}x_i}{n}$$
   $$\mu = \frac{\sum_{i=1}^{n}freq_i \cdot x_i}{\sum_{i=1}^{n}freq_i}$$
   $$\mu = \sum_{i=1}^{n}p_i\cdot x_i$$
** mode
** median
   - even
     $$\frac{x_\frac{n}{2} + x_{\frac{n}{2}+1}}{2}$$

   - odd
     $$x_\frac{n+1}{2}$$

* Variability
** range
   highest value - lowest value

** quartile
   | lower bound(lowest) | lower quartile(Q1) | median(Q2) | upper quartile(Q3) | upper bound(highest) |

*** IQR(Interquartile Range)
    $$Q_3 - Q_1$$
    $Q_1$ is the midean of the first half

    file:../resources/math/IQR.png

*** Outlier
    whiskers = 1.5
    $$< Q_1 - whiskers\cdot(IQR)$$
    $$> Q_3 + whiskers\cdot(IQR)$$

** deviation
*** Average Absolute Deviation
    average absolute distances to the mean
    - The value far away from mean isn't penalized

    eg: set A: 4 4 6 8 8, set B: 2 6 6 6 10, average absolute deviations are the same.

*** MAD(median absolute deviation)
    more robust than variance

*** Variance
    $$\sigma^2 = \frac{\sum_{i=1}^n(x_i-\mu)^2}{n}$$
    $$\sigma^2 = \frac{\sum_{i=1}^n x_i^2}{n} - \mu^2$$
    $$\sigma^2 = \frac{\sum_{i=1}^n(x_i-\mu)^2}p_i$$

*** Standard deviation
    - standard deviation $\sigma$
**** For Normal Distribution
     - approximately 68% lie within 1SD of the mean
     - approximately 95% lie within 2SD of the mean

*** Standardize(z-score)
    $$z=\frac{X-\mu}{\sigma}$$
    Standardize to $X\sim Norm(0, 1)$

* Sample
** Sampling method
*** Simple Random Sampling
    *Simple random sampling* is where you choose sampling units at random
    to form your sample.

*** Stratified Sampling
    *Stratified sampling* is where you divide the population into groups of
    similar units or *strata*. Each *stratum* is as different from the others as possible.
    - these groups are called *strata*
    - each individual group is called *stratum*

*** Cluster Sampling
    *Cluster sampling* is where you divide the population into clusters
    where each cluster is as similar to the others as possible.

*** Systematic Sampling
    *Systematic Sampling* is where you choose a number, k, and sample every kth unit.
** Mean
   $$\bar{x}=\sum_{i=1}^N x_i$$
   - estimate of population mean
   $$\hat\mu=\bar{x}$$

** Variance
   $$s^2 = \frac{\sum_{i=1}^N(x_i-\bar{x})^2}{N}$$
*** estimate of population variance
    Because population distribution is steeper than sample distribution, use Bessel's Correction to get a better estimate of population
    $$\hat{\sigma}^2 = \frac{\sum_{i=1}^N(x_i-\bar{x})^2}{N-1}$$

* Sampling Distribution
** Central limit theorem
1. The distribution of sample mean is approximately normal distribution $\bar{X}\sim Norm(\mu, \frac{\sigma^2}{n})$
2. The standard deviation of the sample means $SD \approx\frac{\sigma}{\sqrt{N}}$ (standard error)
3. The mean of the sample means $\approx\mu$

** Confidence Interval
   - Estimate population mean of Y% confidence interval

    $$(\bar{x}-z\frac{\sigma}{\sqrt{N}}, \bar{x}+z\frac{\sigma}{\sqrt{N}})$$
    $\pm{z}$ are the critical values of Y% confidence interval

   - $z\frac{\sigma}{\sqrt{n}}$ is the margin of error


* Hypothesis testing
- $\alpha$ levels

  Levels of likelihood: 5%, 1%, 0.1%. If the probability of getting a particular
  sample mean is less than 5%, it is unlikely to occur.

- critical region: the aera with the probability is less than particular $\alpha$ level
- critical value: the z-score of the $\alpha$ level

** null hypothesis $H_0$
*** Reject null hypothesis
    - Sample mean falls within the *critical region*
    - z-score of sample mean is *greater than* the z-critical value
    - the probability of obtaining the sample mean is *less than* the $\alpha$ level

*** type 1 error
    Reject $H_0$, but in the real world $H_0$ is true.
    We think our hypothesis is correct, but it is wrong.

*** type 2 error
    Retain $H_0$, but in the real world $H_0$ is false.
    We think our hypothesis is wrong, but it is true.

** alternative hypothesis $H_A$
- assume $H_0$ is
$$\mu\approx\mu_I$$

- then $H_A$ is
$$\mu\neq\mu_I, \mu<\mu_I, \mu>\mu_I$$

* Z-Tests
  - known $\sigma$
  - the sample size is large(often $N \ge 30$)
* T-Tests
  Compare the difference between two means
** SEM(standard error of mean)
   uses sample sd
   $$SEM=\frac{s}{\sqrt{n}}$$
** t-score(one sample)
   $$t=\frac{\bar{x}-\mu_0}{SEM}=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}$$
** Cohen's d
   standardized mean difference that measures the distance between means in standardized units.
   $$Cohen's\ d = \frac{\bar{x}-\mu_0}{s}$$
** $r_2$
   #+BEGIN_VERSE
   $r_2$: coefficient of determination
   $r^2$ % of variation in one variable that is related to
   ('explained by') another variable.
   #+END_VERSE
   $$r^2 = \frac{t^2}{t^2+DF}$$
** Formula Wrapup
$$DF=n-1$$
$$SEM=\frac{s}{\sqrt{n}}$$
$$t=\frac{\bar{x}-\mu}{SEM}$$
$$d=\frac{\bar{x}-\mu}{s}$$

$$margin\ of\ error = t_{criticl}\cdot{SEM}$$
$$CI=\bar{x}\pm{margin\ of\ error}$$
$$r^2=\frac{t^2}{t^2+df}$$

** Terms
| DF  | degree of freedom      |
| SEM | standard error of mean |
* Dependent 2 sample t-test
$$t=\frac{\bar{x}_D-\mu_D}{S_D/\sqrt{n}}$$
$$CI=\bar{x}_D\pm t_{critical}\cdot\frac{S_D}{\sqrt{n}}$$
$$cohen's\ d=\frac{\bar{x}_D}{S_D}$$
** Within-Subject designs
- Two conditions
- Pre-test, post-test
- Growth over time(longitudinal study)
** Effect size
- difference measures: mean, standardized
  #+BEGIN_VERSE
  cohen's d == standardized mean difference
  #+END_VERSE

** Statistical significance

   Statistical significance means:
   - rejected the null
   - results are not likely due to chance(sampling error)

** Advantages
- Controls for individual differences
- Use fewer subject
- Cost-effective
- Less time-consuming
- Less expensive

** Disadvantages
- Carry-over effects

  Second measurement can be affected by first treatment

- Order may influence results

* Independent 2 sample t-test
** Between-Subject designs
- Experimental
- Observational

** DF
   $$DF = n_1+n_2-2$$
** SE
   $$s=\sqrt{s_{1}^2+s_{2}^2}$$

   Assumes samples are approximately the same size, then
   $$SE=\sqrt{\frac{s_{1}^2}{n_1} + \frac{s_{2}^2}{n_2}}$$

*** Corrected Standard Error
    $$SS=\sum_{i=1}^{n}(x_i-\bar{x})^2$$
    $$S_{p}^2=\frac{SS_1 + SS_2}{df_1 + df_2}$$
    $$SE=\sqrt{\frac{S_{p}^2}{n_1} + \frac{S_{p}^2}{n_2}}$$

    cohen's d also uses $S_p$

    $$d=\frac{\bar{x}-\mu}{S_p}$$

** t statistic
   $$t=\frac{\bar{x}_D-\mu_D}{SE}$$

* ANOVA
  Analysis of variance
** Grand mean $\bar{x}_G$
   mean of all values

** F-Ratio
- Between-group variability

  The greater the distance between sample means, the more
  likely population means will differ significantly.

- Within-group variability

  The greater the variability of each individual sample,
  the less likely population means will differ significantly.

$$F=\frac{MS_{between}}{MS_{within}}=\frac{SS_{between}/df_{between}}{SS_{within}/df_{within}}
=\frac{\sum_{i}n_i(\bar{x}_i-\bar{x}_G)^2/(k-1)}{\sum_i\sum_j(x_{ij}-\bar{x}_i)^2/(N-k)}$$
$$SS_{total}=SS_{between}+SS_{within}=\sum_i\sum_j(x_{ij}-\bar{x}_G)$$
$$df_{total}=df_{between}+df_{within}=N-1$$

** Multiple Comparison Tests

   We use *multiple comparison tests* if we want to know which two samples
   are differ after we've done ANOVA.
*** Tukey's Honestly Significant Difference(HSD)
    $$Tukey's HSD = q^*\sqrt{\frac{MS_{within}}{n}} = q^*\frac{S_p}{n}$$
    /q/ is the *Studentized Range Statistic*

** Cohen's d
   $$Cohen's\ d = \frac{\bar{x}_a-\bar{x}_b}{MS_{within}}$$

** Explained Variation $\eta^2$
   Proportion of total variation that is due to between-group differences.
   $$\eta^2=\frac{SS_{between}}{SS_{total}}$$

** ANOVA assumptions
- Normality
- Homogeneity of variance
- Independence of observations

* Report
** Meaningfulness of Results
1. What was measured?
2. Effect size
3. Can we rule out random chance?
4. Can we rule out alternative explanations?(lurking variables)

** descriptive statistics(Mean,SD,...)

   report styles: text, graphs, tables

** inferential statistics($\alpha$)
*** factors
- kind of test
- test statistic
- DF
- p-value
- direction of test(one/two tails)

*** inferential statistics
- confidence intervals
  - confidence level e.g. 95%
  - lower limit
  - upper limit
  - CI on what?

*** effect size measures

    d, $r^2$

*** APA style

    APA style is a whole guide on writing researh papers.
    $$t(df)=xxx, p=xx, direction$$
    example:
    $$t(24)=-2.5, p=0.01, one-tailed$$

    - CI

    example: Confidence interval on the mean difference;95%CI=(4,6)
** visualization
*** Pie charts
    Pie charts work by splitting your data into distinct groups or categories.
    Pie charts are less useful if all the slices have similar sizes, use bar charts for this case.
*** Bar charts
    Bar charts allow you to compare relative sizes, but the advantage of using a bar chart is that they
    allow for a greater degree of precision.
    - vertical or horizontal

    Vertical bar charts tend to be more common, but horizontal bar charts
    are useful if the names of your categories are long.

**** extensions
     - The split-category bar chart
     - The segmented bar chart

*** Histogram
    Histograms are like bar charts but with two key differences.
    - The area of each bar is proportional to the frequency
    - There are no gaps between the bars on the chart

*** Line charts
    Line charts are better at showing a trend.
* Terms
| population mean             | $\mu$                                                             |
| estimate of population mean | $\hat\mu$                                                         |
| population sd               | $\sigma$                                                          |
| estimate of population sd   | $\hat\sigma$                                                      |
| sample mean                 | $\bar{x}$                                                         |
| sample sd                   | $s$                                                               |
| sample size                 | $N$                                                               |
| z-score                     | number of standard deviations away from $\mu$                     |
| sampling distribution       | distribution of sample means                                      |
| standard error              | $SD$ the standard deviation of means of the sampling distribution |

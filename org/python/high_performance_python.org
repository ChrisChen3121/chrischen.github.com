#+TITLE: High Performance Python
#+KEYWORDS: python, performance
#+OPTIONS: H:3 toc:2 num:3 ^:nil
#+LANGUAGE: en-US
#+AUTHOR: ChrisChen
#+EMAIL: ChrisChen3121@gmail.com
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

* Hardware Resources
** Computing Unit
   The main properties of interest in a computing unit are the number of operations
   it can do in one cycle and the number of cycles it can do in one second. The first
   value is measured by its instructions per cycle(IPC), while the latter value is
   measured by its clock speed.
   - *SIMD*: Single instruction, multiple data. Efficient with high IPC. (CPU vectorization instruction)
   - *Hyperthreading* presents a virtual second CPU to the host OS, and clever hardware logic tries
   to interleave two threads of instructions into the execution units on a single CPU. When successful,
   gains of up to 30% over a single thread can be achieved.

** Memory Unit
   When trying to optimize the memory patterns of a program, we are simply optimizing
   which data is placed where, how it is laid out (in order to increase the number of
   sequential reads), and how many times it is moved among the various locations.

** Communications Layers
   - The /frontside/ bus is the connection between the RAM and the L1/L2 cache
   - Two main properties of a bus:
     - how much data can be moved in one transfer (bus width)
     - how many transfers the bus can do per second (bus frequency).

** GPU
   - Drawbacks: Communicates through the PCI bus, which is much slower than the frontside bus

** Performance Issues of Python
   - Memory Fragmentation: due to garbage-collected
   - Dynamic Types: Using Cython to mitigate this problem
   - GIL: Using Cython or foreign functions

* Profiling
** ~timefn~ decorator
   #+BEGIN_SRC python
     def timefn(fn):
	 @wraps(fn)
	 def measure_time(*args, **kwargs):
	     t1 = time.time()
	     result = fn(*args, **kwargs)
	     t2 = time.time()
	     print(f"@timefn: {fn.__name__} took {t2 - t1} seconds")
	     return result
	 return measure_time
   #+END_SRC

** bulit-in ~timeit~ module
   - [[https://docs.python.org/3/library/timeit.html][Documentation]]
   - The ~timeit~ module temporarily disables the garbage collector.
   - More useful approach: IPython magic ~%timeit~

** CProfile
   #+BEGIN_SRC sh
     python -m cProfile -s cumulative test.py
     python -m cProfile -o profile.stats test.py
   #+END_SRC
   #+BEGIN_SRC python
     import pstats
     p = pstats.Stats("profile.stats")
     p.sort_stats("cumulative")
     p.print_stats()
     p.print_callers()
     p.print_callees()
   #+END_SRC

* TODO Profiling
** CPU
*** simple /time/ module
  #+BEGIN_SRC python
    import time
    start_time = time.time()
    fn()
    end_time = time.time()
    print end_time - start_time #seconds
  #+END_SRC
*** decorator
  #+BEGIN_SRC python
    from functools import wraps
    def timefn(fn):
	@wraps(fn)
	def measure_time(*args, **kwargs):
	    t1 = time.time()
	    result = fn(*args, **kwargs)
	    t2 = time.time()
	    print t2 - t1
	    return result
	return measure_time

    @timefn
    def measure_fn(arg1, arg2):
	...
  #+END_SRC
*** %timeit in IPython
*** unix /time/ command
  might be useful if you start lots of fresh processes.
  #+BEGIN_SRC sh
    /usr/bin/time -p python xxx.py
    /usr/bin/time --verbose python xxx.py
  #+END_SRC

*** cProfile
  #+BEGIN_SRC sh
    python -m cProfile -s cumulative xxx.py
  #+END_SRC
  -s cumulative: sort by cumulative time spent.

  - record profiling:
  #+BEGIN_SRC sh
    python -m cProfile -o profile.stats xxx.py
  #+END_SRC
  #+BEGIN_SRC python
    import pstats
    p = pstats.Stats("profile.stats")
    p.sort_stats("cumulative")
    p.print_stats()
    p.print_callers() #locate the most expensive parents
    p.print_callees()
  #+END_SRC
*** visualize a profile file
**** kcachegrind
    #+BEGIN_SRC sh
    pyprof2calltree -i profile.stats -o prof.calltree
    kcachegrind prof.calltree
    #+END_SRC
**** snakeviz
   #+BEGIN_SRC sh
   snakeviz profile.stats
   #+END_SRC

*** line_profiler
** memory
*** memory_profiler
  #+BEGIN_SRC sh
    python -m memory_profiler xxx.py
    #or
    mprof run xxx.py
    mprof plot
  #+END_SRC
*** heapy

  /pip install guppy/ first.
  Add following to your code.
  #+BEGIN_SRC python
    from guppy import hpy; hp = hpy()
    hp.setrelheap()#set checkpoint
    print hp.heap()#print heap usage since last checkpoint
  #+END_SRC
*** TODO dowser??
** work with UT
Add following at the top of unittest code.
#+BEGIN_SRC python
  # for line_profiler
  if '__builtin__' not in dir() or not hasattr(__builtin__, 'profile'):
      def profile(func):
	  def inner(*args, **kwargs):
	      return func(*args, **kwargs)
	  return inner

  # for memory_profiler
  if 'profile' not in dir():
      def profile(func):
	  def inner(*args, **kwargs):
	      return func(*args, **kwargs)
	  return inner
#+END_SRC

* Lists and tuples
** Sort algorithm
*** Tim sort
built-in sort algorithm
(it hybridizes insertion and merge sort algorithms).

*** bisect
    *bisect* provides support for maintaining a list in
    sorted order without having to sort the list after each insertion.
    #+BEGIN_SRC python
      import bisect
      alist=[]
      bisect.insort(alist, 5)
      bisect.insort(alist, 3)
      bisect.insort(alist, 20)
      bisect.insort(alist, 17)
      print alist
      #=> [3, 5, 17, 20]
    #+END_SRC

** list vs. tuple
*** list
dynamic arrays, mutable and allow for resizing.

**** resizing
     The growth pattern is:
     | new size      | 0 | 1 | 5 |  9 | 17 | 26 | 36 | 47 | ... |
     | new allocated | 0 | 4 | 8 | 16 | 25 | 35 | 46 | 58 | ... |
  #+BEGIN_SRC c
    new_allocated = (newsize >> 3) + (newsize < 9 ? 3 : 6);
    new_allocated += newsize;
  #+END_SRC

**** dereference
     List objects (for background, see Chapter 3) have an overhead for each dereference, as
     the objects they reference can occur anywhere in memory.

*** tuple
    static arrays, immutable
- instantiating a list can be 5.1x slower than instantiating a tuple
- tuple is a hashable type

* set & dict
** hashable key
   The type should implements both the __hash__ magic function and either __eq__ or __cmp__ .
*** probing function
    #+BEGIN_SRC python
      # pseudocode
      # mask is always equal to bin(hashtable_size - 1)
      def index_sequence(key, mask=0b111, PERTURB_SHIFT=5):
	  perturb = hash(key)
	  i = perturb & mask
	  yield i
	  while True:
	      i = ((i << 2) + i + perturb + 1)
	      perturb >>= PERTURB_SHIFT
	      yield i & mask
    #+END_SRC
*** User-defined classes
    User-defined classes have default hash and comparison functions.
    The default __hash__ function simply returns the object’s placement
    in memory as given by the built-in id function. Similarly,
    the __cmp__ operator compares the numerical value of the object’s
    placement in memory.

*** entropy
    “how well distributed my hash function is” is called the *entropy*
    of the hash function:
    $$S = - \sum_i p(i)\cdot\log(p(i))$$

    where p(i) is the probability that the hash function gives hash i.

    knowing up front what range of values will be used and how large
    the dictionary will be helps in making a good selection.

** resizing

**** The growth pattern is:

     8, 32, 128, 512, 2048, 8192, 32768, 131072, 262144, ...
     the number of bucket increases by 4x until we reach 50,000
     elements, after which the size is increased by 2x.

     resizing requires recomputing indices
** extra
*** Namespace lookups
  #+BEGIN_SRC python
    import math
    from math import sin
    def test1(x):
	"""
	>>> %timeit test1(123456)
	1000000 loops, best of 3: 381 ns per loop
	"""
	return math.sin(x)

    def test2(x):
	"""
	>>> %timeit test2(123456)
	1000000 loops, best of 3: 311 ns per loop
	"""
	return sin(x)

    def test3(x, sin=math.sin):
	"""
	>>> %timeit test3(123456)
	1000000 loops, best of 3: 306 ns per loop
	"""
	return sin(x)
  #+END_SRC
  #+BEGIN_SRC python
    dis.dis(test1)
    # 0 LOAD_GLOBAL      0 (math)  # Dictionary lookup
    # 3 LOAD_ATTR        1 (sin)   # Dictionary lookup
    # 6 LOAD_FAST        0 (x)     # Local lookup
    # 9 CALL_FUNCTION    1
    # 12 RETURN_VALUE

    dis.dis(test2)
    # 0 LOAD_GLOBAL      0 (sin)   # Dictionary lookup
    # 3 LOAD_FAST        0 (x)     # Local lookup
    # 6 CALL_FUNCTION    1
    # 9 RETURN_VALUE

    dis.dis(test3)
    # 0 LOAD_FAST        1 (sin)   # Local lookup
    # 3 LOAD_FAST        0 (x)     # Local lookup
    # 6 CALL_FUNCTION    1
    # 9 RETURN_VALUE
  #+END_SRC

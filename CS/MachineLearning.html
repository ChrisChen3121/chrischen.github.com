<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Machine Learning</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="Machine Learning"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2014-08-27 17:02:08 中国标准时间"/>
<meta name="author" content="ChrisChen"/>
<meta name="description" content=""/>
<meta name="keywords" content="Machine Learning"/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="../css/stylesheet.css" type="text/css"/>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "left",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div>


<div id="content">
<h1 class="title">Machine Learning</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Definition</a></li>
<li><a href="#sec-2">2 Linear Regression</a>
<ul>
<li><a href="#sec-2-1">2.1 Gradient Descent</a></li>
<li><a href="#sec-2-2">2.2 Normal Equation</a></li>
<li><a href="#sec-2-3">2.3 Polynormial Regression</a></li>
</ul>
</li>
<li><a href="#sec-3">3 Logistic Regression</a>
<ul>
<li><a href="#sec-3-1">3.1 Gradient Descent</a></li>
<li><a href="#sec-3-2">3.2 Other Method to Compute minJ</a></li>
<li><a href="#sec-3-3">3.3 fminunc</a></li>
</ul>
</li>
<li><a href="#sec-4">4 Regularization</a></li>
<li><a href="#sec-5">5 Octave Usage</a></li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Definition</h2>
<div class="outline-text-2" id="text-1">


<p class="verse">
A computer program is said to learn from experience <i>E</i><br/>
with respect to some task <i>T</i> and some performance measure <i>P</i> ,<br/>
if its performance on <i>T</i> , as measured by P, improves with <br/>
experience E.<br/>
</p>


</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Linear Regression</h2>
<div class="outline-text-2" id="text-2">

<ul>
<li>Dimension

<p class="verse">
\(X: m\times(n+1)\) <br/>
\(x^{(i)}: (n+1)\times1\) <br/>
\(\theta: (n+1)\times1\) <br/>
\(y: m\times1\) <br/>
</p>


</li>
<li>Hypothesis
  $$h_\theta(x^{(i)})=\theta^T x^{(i)}$$
  [Matrix] \(h_\theta(X)=X\cdot\theta\)

</li>
<li>Cost Function
  $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
  [Vectorized] \(J(\theta) = \frac{1}{2m}sum((h_\theta(X) - y). \widehat{} 2)\)

</li>
<li>Goal
  $$minimize J(\theta_0, \theta_1)$$
</li>
</ul>



</div>

<div id="outline-container-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Gradient Descent</h3>
<div class="outline-text-3" id="text-2-1">


</div>

<div id="outline-container-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> Outline</h4>
<div class="outline-text-4" id="text-2-1-1">

<ul>
<li>Start with some \(\theta\)
</li>
<li>Keep changing \(\theta\) to reduce \(J(\theta)\), until we hopefully end up at minimum
</li>
</ul>


</div>

</div>

<div id="outline-container-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> Algorithm</h4>
<div class="outline-text-4" id="text-2-1-2">


<p class="verse">
repeat until convergence {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \(\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)\)<br/>
}<br/>
\(\alpha\) is called learning rate. <br/>
</p>


<ul>
<li>Simultaneous Update
  $$temp0 := \theta_0 - \alpha\frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1)$$
  $$temp1 := \theta_1 - \alpha\frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1)$$
  $$\theta_0 := temp0$$
  $$\theta_1 := temp1$$
</li>
</ul>



<p class="verse">
After derivative,<br/>
$$\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$
simultaneously updata \(\theta_j\) for all j<br/>
[Vectorized] \(\theta = \theta - \alpha\frac{1}{m}\cdot X^T \cdot (h_\theta(X) - y)\)<br/>
</p>


</div>

</div>

<div id="outline-container-2-1-3" class="outline-4">
<h4 id="sec-2-1-3"><span class="section-number-4">2.1.3</span> Feature Scaling</h4>
<div class="outline-text-4" id="text-2-1-3">

<ul>
<li>Idea: to make contour map not shape.

<p class="verse">
Because \(x_0=1\), scale \(-1&lt;x_i&lt;1\)<br/>
-3 to 3, \(-\frac{1}{3}\) to \(\frac{1}{3}\) are also good.<br/>
</p>


</li>
<li>Mean normalization
</li>
</ul>


$$x_i = \frac{x_i - \mu_i}{range}$$
$$-0.5\le x_i \le 0.5$$
</div>

</div>

<div id="outline-container-2-1-4" class="outline-4">
<h4 id="sec-2-1-4"><span class="section-number-4">2.1.4</span> Learning Rate</h4>
<div class="outline-text-4" id="text-2-1-4">

<ul>
<li>Correctness

<p>
  \(J(\theta)\) should decrease after every iteration.
</p></li>
</ul>


<p class="verse">
One way:<br/>
Run an automatic convergence test.<br/>
Declare convergence if \(j(\theta)\) decreases by less than \(10^{-3}\) <br/>
in one iteration.<br/>
<br/>
or check the \(iterations-J(\theta)\)  plot<br/>
<br/>
if gradient decent not working, using smaller \(\alpha\)<br/>
But if \(\alpha is too small, gradient descent can be slow to converge\)<br/>
<br/>
To choose \(\alpha\), try:<br/>
&hellip;, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1,&hellip;.<br/>
</p>



</div>
</div>

</div>

<div id="outline-container-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Normal Equation</h3>
<div class="outline-text-3" id="text-2-2">

<ul>
<li>Idea: to compute \(\frac{\partial}{\partial\theta_j}j(\theta_j) = 0\)
</li>
</ul>


$$\theta = (X^TX)^{-1}X^Ty$$
<p>
Octave version: 
</p>


<pre class="src src-octave">pinv(X'*X)*X'*y
</pre>


</div>

<div id="outline-container-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Inverse in Octave</h4>
<div class="outline-text-4" id="text-2-2-1">


<p class="verse">
pinv vs. inv<br/>
pinv is a pseudo-inverse, always make correct result even the matrix is non-invertable.<br/>
</p>


<ul>
<li>Why non-invertable?
<ul>
<li>Redundant features(linearly dependant).
</li>
<li>Too many features

<p class="verse">
Delete some features, or use <a href="#sec-4">regularization</a><br/>
</p>


</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> vs. Gradient Descent</h4>
<div class="outline-text-4" id="text-2-2-2">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">Gradient Descent</th><th scope="col" class="left">Normal Equation</th></tr>
</thead>
<tbody>
<tr><td class="left">Need to choose \(\alpha\)</td><td class="left">No need to choose \(\alpha\)</td></tr>
<tr><td class="left">Needs more iterations</td><td class="left">Don't need to iterate</td></tr>
<tr><td class="left">Works well with many features</td><td class="left">n features, \((X^TX)^{-1}\), takes \(O(n^3)\)</td></tr>
<tr><td class="left">Support various module</td><td class="left">Only support linear regression module</td></tr>
</tbody>
</table>


<p>
Number of features n &gt; 10000, using gradient descent.
</p>

</div>
</div>

</div>

<div id="outline-container-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Polynormial Regression</h3>
<div class="outline-text-3" id="text-2-3">


<p class="verse">
With one feature, the hypothesis looks like:<br/>
\(h_\theta(x)=\theta_0+\theta_1x^2+\theta_2x^3\) <br/>
Let \(x_1=x^2\) , \(x_2=x^3\) , now it's a linear regression problem.<br/>
Note that, in this case feature scaling is necessary.<br/>
</p>


</div>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Logistic Regression</h2>
<div class="outline-text-2" id="text-3">

<ul>
<li>Logistic Function(sigmoid function)
  $$g(x) = \frac{1}{1+e^-x}$$
  if \(x\ge 0\), \(g(x) \ge 0.5\).

</li>
<li>Hypothesis
  $$h_\theta(x)=g(\theta^T x) = \frac{1}{1+e^{-\theta^T x}}$$

</li>
<li>Cost function
  $$Cost(h_\theta(x), y) = \begin{cases}
  -log(h_\theta(x)),  & \mbox{if } y=1 \\
  -log(1-h_\theta(x)), & \mbox{if } y=0 \\
  \end{cases}$$
  Simplification:
  $$Cost(h_\theta(x), y) = -y log(h_\theta(x)) - (1-y)log(1-h_\theta(x))$$

</li>
<li>\(J(\theta)\)
  $$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}, y^{(i)}))$$
</li>
</ul>



</div>

<div id="outline-container-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Gradient Descent</h3>
<div class="outline-text-3" id="text-3-1">


<p class="verse">
$$\theta_j=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(\frac{1}{1+e^{-\theta^Tx^{(i)}}}-y^{(i)})x_j^{(i)}$$
(simultaneously update for all j)<br/>
[Vectorized] \(\theta=\theta-\alpha\frac{1}{m}X^T(h_\theta(X)-y)\)<br/>
</p>


</div>

</div>

<div id="outline-container-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Other Method to Compute minJ</h3>
<div class="outline-text-3" id="text-3-2">

<p>Conjugate Gradient, BFGS, LBFGS
</p>
</div>

</div>

<div id="outline-container-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> fminunc</h3>
<div class="outline-text-3" id="text-3-3">




<pre class="src src-octave"><span style="color: #1e90ff; font-weight: bold;">function</span> [jVal, gradient] = <span style="color: #00008b; font-weight: bold;">costFunction</span>(theta)
jVal = [...code to compute <span style="color: #008b8b;">J</span>(theta)...];
gradient = [...code to compute derivative of <span style="color: #008b8b;">J</span>(theta)...];
<span style="color: #1e90ff; font-weight: bold;">end</span>

options = optimset(<span style="color: #9400d3;">'GradObj'</span>, <span style="color: #9400d3;">'on'</span>, <span style="color: #9400d3;">'MaxIter'</span>, <span style="color: #9400d3;">'100'</span>);
initialTheta = zeros(2,1);
[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
</pre>


</div>

<div id="outline-container-3-3-1" class="outline-4">
<h4 id="sec-3-3-1"><span class="section-number-4">3.3.1</span> gradient</h4>
<div class="outline-text-4" id="text-3-3-1">


$$\frac{\partial J(\theta)}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$
<p>
[Vectorized] \(\frac{\partial J(\theta)}{\partial\theta}=\frac{1}{m}X^T(h_\theta(X)-y)\)
</p>
</div>
</div>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4"><a name="regularization" id="regularization"></a><span class="section-number-2">4</span> Regularization</h2>
<div class="outline-text-2" id="text-4">


</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Octave Usage</h2>
<div class="outline-text-2" id="text-5">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">Function</th><th scope="col" class="left">Description</th></tr>
</thead>
<tbody>
<tr><td class="left">eye()</td><td class="left">identity matrix</td></tr>
<tr><td class="left">plot()</td><td class="left">generate graph</td></tr>
</tbody>
</table>

</div>
</div>
</div>

</body>
</html>
